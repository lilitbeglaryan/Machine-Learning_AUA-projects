{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from random import randrange\n",
    "# import numpy as np\n",
    "\n",
    "# from scipy.spatial.distance import cdist\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# import matplotlib.pyplot as plt\n",
    "# # from sklearn.model_selection import KFold\n",
    "# # from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# def generate_data(num_samples, num_features):\n",
    "#     \"\"\"\n",
    "#     Generates a synthetic dataset with a large number of features and a small number of samples using\n",
    "#     a Gaussian distribution.\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     num_samples : int\n",
    "#         The number of samples to generate.\n",
    "#     num_features : int\n",
    "#         The number of features for each sample.\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     X : ndarray of shape (num_samples, num_features)\n",
    "#         The input features for each sample.\n",
    "#     y : ndarray of shape (num_samples,)\n",
    "#         The output labels for each sample.\n",
    "#     \"\"\"\n",
    "#     # Generate input features with a Gaussian distribution.\n",
    "#     X = np.random.randn(num_samples, num_features)\n",
    "\n",
    "#     # Generate random output labels.\n",
    "#     y = np.random.randint(0, 2, size=num_samples)\n",
    "\n",
    "#     return X, y\n",
    "\n",
    "\n",
    "# class KNNClassifier:\n",
    "#     \"\"\"\n",
    "#     K-nearest neighbors classifier.\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     n_neighbors : int, optional (default=5)\n",
    "#         Number of neighbors to use for classification.\n",
    "#     metric : str or callable, optional (default='euclidean')\n",
    "#         Distance metric to use for computing the distances between samples.\n",
    "#         Supported metrics include 'euclidean', 'manhattan', 'chebyshev', and\n",
    "#         any other metric supported by the `scipy.spatial.distance.cdist` function.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, n_neighbors=5, metric='euclidean'):\n",
    "#         self.n_neighbors = n_neighbors\n",
    "#         self.metric = metric\n",
    "\n",
    "#     def fit(self, X, y):\n",
    "#         \"\"\"\n",
    "#         Fit the KNN classifier to the training data.\n",
    "\n",
    "#         Parameters\n",
    "#         ----------\n",
    "#         X : ndarray of shape (n_samples, n_features)\n",
    "#             The training input samples.\n",
    "#         y : ndarray of shape (n_samples,)\n",
    "#             The target values.\n",
    "#         \"\"\"\n",
    "\n",
    "#         assert len(X) == len(y) #should be of the same size\n",
    "\n",
    "#         self.X_tr = X\n",
    "#         self.y_tr = y\n",
    "    \n",
    "#         # raise NotImplementedError\n",
    "\n",
    "#     def predict(self, X):\n",
    "#         \"\"\"\n",
    "#         Predict the class labels for the input samples.\n",
    "\n",
    "#         Parameters\n",
    "#         ----------\n",
    "#         X : ndarray of shape (n_samples, n_features)\n",
    "#             The input samples.\n",
    "\n",
    "#         Returns\n",
    "#         -------\n",
    "#         y_pred : ndarray of shape (n_samples,)\n",
    "#             The predicted class labels.\n",
    "#         \"\"\"\n",
    "#         y_pred = []\n",
    "#         for row in X: #for each new test observation(row)\n",
    "#             dists = [self.dist(row, x_train_row) for x_train_row in self.X_tr] #compute the given metric distance from each train row\n",
    "#             k_neigh = np.argsort(dists)[:self.n_neighbors]\n",
    "#             k_classes = [self.y_tr[idx] for idx in k_neigh]\n",
    "#             y_pred.append(np.argmax(np.bincount(k_classes))) #return the dominating label\n",
    "#         return y_pred\n",
    "#         # raise NotImplementedError\n",
    "\n",
    "#     def euclid_dist(self, row1, row2):\n",
    "#         sum_squared_distance = np.sum((row1 - row2)**2)\n",
    "#         return np.sqrt(sum_squared_distance)\n",
    "#     def manhat_dist(self, vector1, vector2):\n",
    "#         if len(vector1) != len(vector2):\n",
    "#                 raise ValueError(\"Undefined for sequences of unequal length.\")\n",
    "#         return np.abs(np.array(vector1) - np.array(vector2)).sum()\n",
    "\n",
    "#     def cheb_dist(self, vector1, vector2):\n",
    "#         if len(vector1) != len(vector2):\n",
    "#                 raise ValueError(\"Undefined for sequences of unequal length.\")\n",
    "#         return np.max(np.abs(np.array(vector1) - np.array(vector2)))\n",
    "\n",
    "#     def dist(self,r1,r2):\n",
    "#         if(self.metric == \"euclidean\"):\n",
    "#             return self.euclid_dist(r1,r2)\n",
    "#         if(self.metric == \"chebyshev\"):\n",
    "#             return self.cheb_dist(r1,r2)\n",
    "#         else: #manhattan\n",
    "#             return self.manhat_dist(r1,r2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def evaluate_knn_performance(X, y, estimator, k_values, n_folds=5, metric='euclidean'):\n",
    "#     \"\"\"Evaluates the performance of the KNN classifier using k-fold cross-validation.\n",
    "\n",
    "#     Parameters:\n",
    "#     -----------\n",
    "#     X : array-like of shape (n_samples, n_features)\n",
    "#         The input samples.\n",
    "#     y : array-like of shape (n_samples,)\n",
    "#         The target values.\n",
    "#     estimator :\n",
    "#         Your or sklearn's model\n",
    "#     k_values : array-like\n",
    "#         The values of k to use for the KNN classifier.\n",
    "#     n_folds : int, optional (default=5)\n",
    "#         The number of folds to use for cross-validation.\n",
    "#     metric : str, optional (default='euclidean')\n",
    "#         The distance metric to use for the KNN classifier.\n",
    "\n",
    "#     Returns:\n",
    "#     --------\n",
    "#     accuracies : dict\n",
    "#         A dictionary containing the accuracy of the KNN classifier for each value of k on validation set.\n",
    "#         You can interpret this as average value.\n",
    "\n",
    "#     NOTE: you can return other values if you need to.\n",
    "#     \"\"\"\n",
    "\n",
    "#     #I have implemented the k-fold and didn't use the python's standard one\n",
    "#     dataSplit = list()\n",
    "#     dataCopy = list(X)\n",
    "#     foldSize = int(len(X) / n_folds) #get #of each fold\n",
    "#     for _ in range(n_folds): # repeat for all folds\n",
    "#         fold = list()\n",
    "#         while len(fold) < foldSize:\n",
    "#             index = randrange(len(dataCopy))\n",
    "#             fold.append(dataCopy.pop(index)) #remove the foldsize numebr of rowss from X and add to the current fold\n",
    "            \n",
    "#         dataSplit.append(fold) # add the obtained fold to the list\n",
    "    \n",
    "#     #now dataSplit is containing the n_folds data\n",
    "#     # knn = KNNClassifier(metric=metric)\n",
    "#     for k in k_values:\n",
    "        \n",
    "#         scores = list()\n",
    "#         fold = list()\n",
    "#         for fold in dataSplit:\n",
    "#             trainSet = list(dataSplit)\n",
    "#             trainSet.remove(fold) #keep the current fold for test, others for the training\n",
    "#             trainSet = sum(trainSet, [])\n",
    "#             testSet = list()\n",
    "#             for row in fold:\n",
    "#                 rowCopy = list(row)\n",
    "#                 testSet.append(rowCopy)\n",
    "\n",
    "#             #now fit the train set \n",
    "#             trainLabels = [row[-1] for row in trainSet]\n",
    "#             trainSet = [train[:-1] for train in trainSet]\n",
    "#             estimator.fit(trainSet,trainLabels)     \n",
    "\n",
    "#             # and test on the current fold\n",
    "#             actual = [row[-1] for row in testSet] #the actual label \n",
    "#             testSet = [test[:-1] for test in testSet]\n",
    "\n",
    "#             pred = estimator.predict(testSet)\n",
    "                \n",
    "#             accuracy = printMetrics(actual, pred)\n",
    "#             scores.append(accuracy)\n",
    "\n",
    "#         print(\"for k = %d\" % k)\n",
    "#         print('Scores %s' % scores)\n",
    "#         print('-'*20)        \n",
    "#         print('\\nMaximum Accuracy: %3f%%' % max(scores))\n",
    "#         print('\\nMean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))\n",
    "\n",
    "# def plot_learning_curve(k, *args, **kwargs):\n",
    "#     \"\"\"\n",
    "#     Plot the learning curve for a KNN classifier using different values of k.\n",
    "\n",
    "#     NOTE: You can modify arguments as you like, except `k`.\n",
    "\n",
    "#     Parameters:\n",
    "#     ----------\n",
    "#     k : int or array like\n",
    "\n",
    "#     Returns:\n",
    "#     -------\n",
    "#     None\n",
    "#     \"\"\"\n",
    "#     raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k is 1\n",
      "[[array([ 6.05946906e-01,  2.38568275e-01, -6.70666079e-01, -4.24091588e-01,\n",
      "       -3.51653844e-01,  2.66806105e-01, -6.92082918e-04, -1.02039824e+00,\n",
      "        8.27751811e-01, -9.29794342e-01]), array([-0.70033068,  0.87294425, -0.88188194, -1.00094757, -0.04314817,\n",
      "        1.35539574,  1.49868164,  1.1523614 ,  1.17895182, -0.70722893]), array([ 0.18828957,  0.26689697,  1.22064867, -0.47887294,  0.84510291,\n",
      "       -0.52361723, -1.06901488, -0.67807869,  0.22561698, -1.45520045]), array([-0.88983156,  1.95018077,  0.37442897,  0.44259272,  0.11503722,\n",
      "        0.37380749,  0.31912774, -0.77375352, -1.68682826,  1.79305462]), array([ 0.85195171, -0.95983109,  0.84341659, -0.95350358,  1.15579333,\n",
      "        1.20293179,  0.38871026,  1.36853313, -0.49843635, -0.56918144]), array([ 1.39264281,  2.03449163, -0.15092766, -1.55733109, -0.98177228,\n",
      "        1.04324153,  0.36239839, -1.95523758,  0.92172755,  0.56672176]), array([-0.01228873,  0.39361623, -0.39185649, -1.25665489, -0.0981587 ,\n",
      "        0.18398691, -0.60948812,  0.45369144, -0.25107745, -0.48184469]), array([ 1.00785426,  0.90456017,  1.60316259,  0.76860398, -0.38092873,\n",
      "        2.33582521,  1.19515615,  0.37268161, -2.19693381,  0.06196819]), array([ 0.23363319, -0.04230837,  0.96725668,  1.29136436, -0.34446127,\n",
      "        0.16178456, -1.11309529,  1.13386607, -0.20666061,  1.07656748]), array([-2.11141898,  0.85323117,  1.38928197,  0.52474553,  1.19895935,\n",
      "        0.22759484, -0.72908544, -1.22624875,  0.20799667,  0.05269729]), array([ 0.38125953,  0.91437564, -0.13249357,  1.60401738,  0.47847838,\n",
      "        0.49065924,  0.56210481, -0.50573441,  0.72075533,  1.04991326]), array([-1.83841496,  0.79053294, -0.92350757,  1.70965222, -0.23302565,\n",
      "        0.8919837 ,  0.899675  ,  1.1137734 ,  0.65998928, -1.28193793]), array([ 0.11275295,  0.45525792,  0.86246495, -1.49537122,  0.66304055,\n",
      "        0.46614489,  0.2439408 , -0.46596311, -0.85469399,  0.57453214]), array([ 0.76425545,  2.31794995, -2.08860969,  1.92478883, -0.75961083,\n",
      "        0.64977461, -0.79591239,  0.73079837,  0.93051564, -1.28172482]), array([ 0.80386803,  0.65513424,  0.29812075,  1.14562798, -0.14432043,\n",
      "       -1.69531881, -0.91945646,  0.52163548,  1.4807774 ,  0.70704581]), array([ 0.07604111,  1.54461896,  0.69270741, -0.17992469, -1.32713641,\n",
      "        0.81436397,  1.09426705,  0.51073001, -0.50411249,  0.17974461]), array([-0.72038625,  0.97158568, -1.52501341,  1.09417103, -0.06785635,\n",
      "       -1.15323735,  1.90177359,  0.77120124, -0.5889785 ,  0.57000854]), array([ 0.71431052,  0.69900019,  2.07872392, -0.5433456 ,  1.70015364,\n",
      "        0.67453642,  0.17443622, -0.7124714 , -1.07578616,  0.10963617]), array([-1.37900609,  1.33092409,  1.31907606,  0.0655452 , -0.08233939,\n",
      "        0.76692099,  1.14573805, -0.83263635, -2.33513453, -1.00370802]), array([-0.88745008,  0.30631959, -0.44160689,  0.16678633,  2.60553765,\n",
      "        0.64251124,  1.57457804, -0.83016638,  0.55610593,  1.41291596])], [array([-0.6169193 ,  0.19438546, -0.37620415, -1.05757417,  0.27832527,\n",
      "        2.4152333 , -2.06756537,  0.14891946, -1.00986655,  2.54348036]), array([-0.14109038, -1.10046756,  0.7560492 , -0.31557794, -1.41870867,\n",
      "        1.09053547, -1.21338544, -0.15276643, -2.12897137, -1.35277871]), array([-0.55905495,  2.35277878,  1.15597565, -1.11339209, -0.85134758,\n",
      "        0.28924025, -0.84460939,  0.08696766,  0.62827688, -0.53478571]), array([ 0.17451358, -0.05337696,  0.80029694,  0.43819497, -0.88623601,\n",
      "       -1.10125408, -1.08766975, -0.68803702, -1.36557945,  0.52429775]), array([ 0.81423183,  0.16456055,  0.00282539, -1.94866697, -0.93926842,\n",
      "        0.43466332,  0.55908292,  0.90161774,  0.09460208,  0.27165117]), array([-1.68286632e+00, -5.57984212e-01, -1.66574460e-02, -2.60480416e+00,\n",
      "       -5.10887750e-01, -1.34921596e-01,  9.92246838e-01,  3.56747653e-04,\n",
      "        1.20644432e+00,  4.51656611e-01]), array([ 0.49710303, -2.34163931,  0.83914125, -1.97905137,  1.19008613,\n",
      "        0.33476917,  0.82541608, -0.93303241, -0.59009429,  0.61586514]), array([ 2.32494729,  0.3063737 , -0.46202353, -0.18978841,  1.01782364,\n",
      "        0.5084799 , -1.52699503,  0.11593649,  1.45951767, -0.60690192]), array([ 0.31952924,  3.26599963,  1.55105931, -0.69943574,  0.35291863,\n",
      "        0.31849275,  1.2056547 ,  0.84942422, -0.2204367 ,  0.49981042]), array([-0.69357096,  1.37209132,  1.20610272,  1.11507213,  1.34527717,\n",
      "        1.48798726,  0.22548324, -0.36925401, -1.30489027,  0.29948656]), array([ 1.43486013, -0.91954873, -0.17807011, -0.28334905, -1.4856804 ,\n",
      "        0.61247391, -0.73788639, -1.20888951,  0.73861207, -1.23367174]), array([-0.1200858 ,  1.32461828,  1.50019272, -1.21203322, -0.27967656,\n",
      "        0.22045181,  0.2473333 , -1.1357106 , -0.57877281,  0.59631044]), array([-0.73462389,  0.01217508,  1.57777248, -0.13498929, -1.92737548,\n",
      "       -0.44650245, -0.14824661, -0.8097781 , -0.47529082, -0.97565235]), array([ 0.28915715,  0.90008834, -0.82667324,  0.52114528, -0.55926312,\n",
      "       -0.2018337 , -1.18257967, -0.20064602, -0.84368192,  1.56244918]), array([-0.9247981 , -0.19037704,  0.84103392,  0.95169459,  1.0836518 ,\n",
      "       -0.1216508 ,  0.16396992, -1.33124389,  0.13742885,  0.1488768 ]), array([-1.39796812, -0.11965404,  0.15643958, -0.55688069,  1.41202997,\n",
      "       -0.72792701,  0.19464824,  0.41384651,  0.27540928,  0.75098746]), array([ 0.37858736, -1.73610019, -0.54259901,  0.97782802, -1.61717033,\n",
      "        0.72979376, -1.56108969,  0.86063423, -0.67235532, -0.84826967]), array([-0.14374879, -1.12333454, -0.86078925,  0.76899393,  0.61268698,\n",
      "       -0.82369106,  1.62202732, -1.4335995 , -1.32141441,  1.36577524]), array([-0.2077277 , -0.41186739, -0.59252524, -0.11886247, -0.17620094,\n",
      "        0.56830178, -0.02265546, -1.11561458, -0.98745372,  1.33474062]), array([-0.29056113, -0.3777922 , -0.63725382, -1.21343508, -0.84542194,\n",
      "        1.19968357, -0.02324946, -0.58732002, -0.83154113, -0.14852807])], [array([-0.37545894, -0.69943981, -0.8228522 , -1.0805628 , -0.56342777,\n",
      "       -1.91087426, -1.12929116, -1.11407865, -0.08312241, -0.16242412]), array([ 0.96150675, -0.57276332, -1.67611754, -0.17075777, -0.15907919,\n",
      "       -0.24815236,  0.14600418, -0.68981331,  0.83829004, -0.81904286]), array([-0.4141823 , -1.04523041,  0.66057161, -0.55248836, -0.49998326,\n",
      "       -1.85774657,  1.10958522, -1.06228873, -0.14122434,  0.12667795]), array([-1.22700691,  0.1032645 , -1.76070886,  0.11194069,  1.13063899,\n",
      "        0.15946002, -0.84695707, -2.6131122 , -2.97729035, -1.10470437]), array([-0.86302017,  0.71123532,  0.58013237, -0.3138975 , -0.58021451,\n",
      "        1.18331023,  0.85124867,  0.29904703, -0.67073161, -0.10344906]), array([-0.49214498, -1.8622123 ,  0.07275969,  1.31815787, -0.60594203,\n",
      "       -1.70390526,  0.25978494,  0.75700556, -2.14783684, -1.0191623 ]), array([-1.49546585,  0.14424066, -0.46047096, -2.31946788,  0.72105102,\n",
      "        1.42385269, -1.73446568, -0.21728945, -1.04819828, -0.58277339]), array([-0.86048733, -1.38580819,  0.18420593,  1.05949405, -0.29488082,\n",
      "       -0.47911209, -0.07996862,  1.2053143 , -1.62159296, -0.09504411]), array([-0.843738  , -0.708519  ,  0.48937238, -0.18288392,  0.17945047,\n",
      "       -1.27319853,  1.19669584,  0.53008048, -0.23652277,  0.60682688]), array([ 0.03445454, -0.35257851, -0.24356205,  0.31849347,  0.81446579,\n",
      "        0.25136695, -0.12204461, -1.23510096,  0.35986361, -1.67570288]), array([ 1.49998738, -0.13332558, -0.52083804, -0.41690025, -1.36012469,\n",
      "       -0.11179511,  0.52356482,  0.03181918, -1.89384057, -1.7002179 ]), array([ 0.12233084, -0.16115356, -0.46457548,  2.07345985, -1.19179407,\n",
      "       -0.42518388, -0.66989051,  0.75944184,  1.08494883,  0.55870729]), array([ 0.85829395, -1.93396021, -0.72591406, -2.35299397,  0.05354754,\n",
      "        0.25669242, -0.59629148,  1.9115194 , -1.62646253, -0.47214332]), array([ 1.04119545,  0.54796113, -1.30577305,  0.42587614, -0.23710883,\n",
      "        1.58462855,  0.99684742, -0.7120839 ,  0.10452281, -1.45523942]), array([-1.64563607,  0.44309597, -0.39246843,  0.77424188, -1.04875442,\n",
      "        1.92876411, -2.80526879, -0.5645521 ,  0.58455198,  0.60592437]), array([-0.11534474,  0.1442667 , -0.17313022,  0.66854871,  1.03048439,\n",
      "       -0.38609549,  0.4743815 , -0.62305611,  0.81682726, -0.45906583]), array([-0.5469218 ,  0.09960145,  0.38800666,  1.27727964, -0.88386825,\n",
      "       -0.38854079, -0.74915516,  0.06692079, -1.02105598,  1.42337066]), array([ 0.34444834,  1.05913923,  0.68969582,  0.20805276, -0.56393544,\n",
      "        1.31042522, -0.57511407,  2.02368302, -2.77033839,  0.80917724]), array([ 2.31137313, -1.09288444, -0.33474581, -0.21986299,  0.33652558,\n",
      "        0.58641111,  1.44887299, -1.33037691,  0.27576303, -0.30299288]), array([-0.81534049, -0.22850432, -0.57418275,  1.19720976, -0.653623  ,\n",
      "        2.27859384, -0.37961952, -1.35302622,  0.70888187, -1.00598064])], [array([-1.48832581,  1.28816906,  0.86318913, -0.3017162 ,  0.59833808,\n",
      "        0.15151899,  1.34002204, -1.27927658, -1.19656337, -1.52209955]), array([ 0.23928124, -0.16246075, -0.90555909,  0.15971147,  0.13130164,\n",
      "       -1.09134876,  0.19638994, -0.96011544, -0.07034984, -0.94846795]), array([ 1.75920678, -0.41160866,  1.39496924, -1.78015715,  0.77402337,\n",
      "       -2.1012034 , -1.23437398,  0.38585522,  0.49886397, -1.2380482 ]), array([ 0.0566676 , -1.83977053,  1.03408534,  0.66489961, -1.88846995,\n",
      "       -0.80666098,  0.16854029, -1.38929533, -0.23980982,  0.11964717]), array([ 0.30889451, -1.13405382, -0.25516866,  0.23465734, -1.36480437,\n",
      "       -0.33154935, -0.39112089, -1.63356385,  0.14375614, -0.29602661]), array([ 0.74331138,  1.0785604 , -0.23701643,  0.12146555,  0.27837545,\n",
      "       -0.41427221,  1.13629393, -0.59026683, -1.04412036, -0.01365847]), array([-0.75932833, -0.43150725,  0.09652111, -0.61421132,  1.55867292,\n",
      "        0.12374298, -0.56617512,  0.78533044, -0.53817426,  0.39274739]), array([-0.4946941 , -0.66134741,  0.80141756,  0.83045857, -0.85101309,\n",
      "       -1.55442055, -0.76177766, -1.72433561,  0.81182493, -0.77035911]), array([ 1.39958503,  0.29453718, -1.00833713, -0.57921037,  1.1484279 ,\n",
      "        1.89931678, -0.00477058,  0.23706765,  0.17280711, -0.42605431]), array([ 1.01344262,  1.0987117 ,  0.62691157, -0.04694749,  0.31023843,\n",
      "       -0.49447015, -1.00649593,  0.24379483,  0.63811185, -0.17733071]), array([ 0.73131797, -1.39361507,  0.78477278, -1.28254133,  0.38371976,\n",
      "        1.03300289, -0.19146358,  1.5380118 , -1.49667856, -1.38549518]), array([ 0.73259497, -0.73884361,  0.43379734, -0.32636637,  0.05337408,\n",
      "       -0.328833  ,  1.13951667, -0.28572482, -1.03790883, -0.55592524]), array([ 2.07400117,  1.17214524, -2.5937992 , -0.47956471, -0.35402632,\n",
      "        1.49704473,  0.69135914, -0.57529781,  1.78029402,  0.42850301]), array([ 1.74474614, -0.73360523, -1.07147243,  0.91679765, -0.88888364,\n",
      "        0.24780178,  1.2972264 ,  0.99734644,  0.00758491, -1.10250009]), array([-1.19635867, -0.38966444,  0.28875065, -1.42185446,  0.44047835,\n",
      "        0.33561865, -2.12947128,  0.32024751, -0.04281781,  0.04420361]), array([-0.58781626, -1.08352331,  0.04174552,  1.0894716 , -0.07750394,\n",
      "       -0.15038887,  0.7750208 , -0.69602992, -1.16388307,  1.2884164 ]), array([ 0.363483  , -0.76543518,  0.63919452, -1.01065656, -1.48313343,\n",
      "       -1.81381736, -0.80466133, -0.86569912, -1.5930849 , -0.40703167]), array([ 1.83000295,  0.14700222,  0.11760734,  2.02957887, -0.4763478 ,\n",
      "        1.51734878,  1.36675573, -0.51572788,  0.66127586, -0.9395438 ]), array([-0.89113463, -1.98277197, -1.77203145, -2.08776567,  0.55497085,\n",
      "       -0.65460575, -1.13052843, -0.67566642,  0.01459336,  0.44864274]), array([-1.11370326,  0.07763812, -0.07201459, -0.48424887, -0.11513951,\n",
      "       -0.68742242, -0.05413385, -0.62964733, -0.27851469, -0.91126656])]]\n",
      "[[0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1], [0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1], [1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0], [1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1]]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m knn\u001b[39m.\u001b[39mfit(X, y)\n\u001b[0;32m      6\u001b[0m k_values\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39marray([\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m3\u001b[39m]) \u001b[39m#k=1,2,3\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m evaluate_knn_performance(X\u001b[39m=\u001b[39;49mX, y\u001b[39m=\u001b[39;49my, estimator\u001b[39m=\u001b[39;49mknn,k_values\u001b[39m=\u001b[39;49mk_values )\n",
      "Cell \u001b[1;32mIn[59], line 191\u001b[0m, in \u001b[0;36mevaluate_knn_performance\u001b[1;34m(X, y, estimator, k_values, n_folds, metric)\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[39mprint\u001b[39m(trainSet)\n\u001b[0;32m    190\u001b[0m     \u001b[39mprint\u001b[39m(ySet)\n\u001b[1;32m--> 191\u001b[0m     \u001b[39mprint\u001b[39m(f1\u001b[39m.\u001b[39;49mshape)\n\u001b[0;32m    193\u001b[0m \u001b[39m#     #now fit the train set \u001b[39;00m\n\u001b[0;32m    194\u001b[0m \u001b[39m#     trainLabels = [row[-1] for row in trainSet]\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[39m#     trainSet = [train[:-1] for train in trainSet]\u001b[39;00m\n\u001b[0;32m    196\u001b[0m     estimator\u001b[39m.\u001b[39mfit(trainSet,ySet)     \n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "\n",
    "X, y = generate_data(100,10)\n",
    "\n",
    "\n",
    "knn = KNNClassifier()\n",
    "knn.fit(X, y)\n",
    "k_values=np.array([1,2,3]) #k=1,2,3\n",
    "evaluate_knn_performance(X=X, y=y, estimator=knn,k_values=k_values )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "import numpy as np\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "# from sklearn.model_selection import KFold\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def generate_data(num_samples, num_features):\n",
    "    \"\"\"\n",
    "    Generates a synthetic dataset with a large number of features and a small number of samples using\n",
    "    a Gaussian distribution.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_samples : int\n",
    "        The number of samples to generate.\n",
    "    num_features : int\n",
    "        The number of features for each sample.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X : ndarray of shape (num_samples, num_features)\n",
    "        The input features for each sample.\n",
    "    y : ndarray of shape (num_samples,)\n",
    "        The output labels for each sample.\n",
    "    \"\"\"\n",
    "    # Generate input features with a Gaussian distribution.\n",
    "    X = np.random.randn(num_samples, num_features)\n",
    "\n",
    "    # Generate random output labels.\n",
    "    y = np.random.randint(0, 2, size=num_samples)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "class KNNClassifier:\n",
    "    \"\"\"\n",
    "    K-nearest neighbors classifier.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_neighbors : int, optional (default=5)\n",
    "        Number of neighbors to use for classification.\n",
    "    metric : str or callable, optional (default='euclidean')\n",
    "        Distance metric to use for computing the distances between samples.\n",
    "        Supported metrics include 'euclidean', 'manhattan', 'chebyshev', and\n",
    "        any other metric supported by the `scipy.spatial.distance.cdist` function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_neighbors=5, metric='euclidean'):\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.metric = metric\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the KNN classifier to the training data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of shape (n_samples, n_features)\n",
    "            The training input samples.\n",
    "        y : ndarray of shape (n_samples,)\n",
    "            The target values.\n",
    "        \"\"\"\n",
    "\n",
    "        assert len(X) == len(y) #should be of the same size\n",
    "\n",
    "        self.X_tr = X\n",
    "        self.y_tr = y\n",
    "    \n",
    "        # raise NotImplementedError\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the class labels for the input samples.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of shape (n_samples, n_features)\n",
    "            The input samples.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y_pred : ndarray of shape (n_samples,)\n",
    "            The predicted class labels.\n",
    "        \"\"\"\n",
    "        y_pred = []\n",
    "        for row in X: #for each new test observation(row)\n",
    "            dists = [self.dist(row, x_train_row) for x_train_row in self.X_tr] #compute the given metric distance from each train row\n",
    "            k_neigh = np.argsort(dists)[:self.n_neighbors]\n",
    "            k_classes = [self.y_tr[idx] for idx in k_neigh]\n",
    "            y_pred.append(np.argmax(np.bincount(k_classes))) #return the dominating label\n",
    "        return y_pred\n",
    "        # raise NotImplementedError\n",
    "\n",
    "    def euclid_dist(self, row1, row2):\n",
    "        sum_squared_distance = np.sum((row1 - row2)**2)\n",
    "        return np.sqrt(sum_squared_distance)\n",
    "    def manhat_dist(self, vector1, vector2):\n",
    "        if len(vector1) != len(vector2):\n",
    "                raise ValueError(\"Undefined for sequences of unequal length.\")\n",
    "        return np.abs(np.array(vector1) - np.array(vector2)).sum()\n",
    "\n",
    "    def cheb_dist(self, vector1, vector2):\n",
    "        if len(vector1) != len(vector2):\n",
    "                raise ValueError(\"Undefined for sequences of unequal length.\")\n",
    "        return np.max(np.abs(np.array(vector1) - np.array(vector2)))\n",
    "\n",
    "    def dist(self,r1,r2):\n",
    "        if(self.metric == \"euclidean\"):\n",
    "            return self.euclid_dist(r1,r2)\n",
    "        if(self.metric == \"chebyshev\"):\n",
    "            return self.cheb_dist(r1,r2)\n",
    "        else: #manhattan\n",
    "            return self.manhat_dist(r1,r2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_knn_performance(X, y, estimator, k_values, n_folds=5, metric='euclidean'):\n",
    "    \"\"\"Evaluates the performance of the KNN classifier using k-fold cross-validation.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array-like of shape (n_samples, n_features)\n",
    "        The input samples.\n",
    "    y : array-like of shape (n_samples,)\n",
    "        The target values.\n",
    "    estimator :\n",
    "        Your or sklearn's model\n",
    "    k_values : array-like\n",
    "        The values of k to use for the KNN classifier.\n",
    "    n_folds : int, optional (default=5)\n",
    "        The number of folds to use for cross-validation.\n",
    "    metric : str, optional (default='euclidean')\n",
    "        The distance metric to use for the KNN classifier.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    accuracies : dict\n",
    "        A dictionary containing the accuracy of the KNN classifier for each value of k on validation set.\n",
    "        You can interpret this as average value.\n",
    "\n",
    "    NOTE: you can return other values if you need to.\n",
    "    \"\"\"\n",
    "\n",
    "    #I have implemented the k-fold and didn't use the python's standard one\n",
    "    dataSplit = list()\n",
    "    y_split=list()\n",
    "    dataCopy = list(X)\n",
    "    yCopy=list(y)\n",
    "    foldSize = int(len(X) / n_folds) #get #of each fold\n",
    "    for _ in range(n_folds): # repeat for all folds\n",
    "        fold1 = list()\n",
    "        fold2 = list()\n",
    "        while len(fold1) < foldSize:\n",
    "            index = randrange(len(dataCopy))\n",
    "            fold1.append(dataCopy.pop(index)) #remove the foldsize numebr of rowss from X and add to the current fold\n",
    "            fold2.append(yCopy.pop(index)) \n",
    "        dataSplit.append(fold1) # add the obtained fold to the list\n",
    "        y_split.append(fold2)\n",
    "\n",
    "    \n",
    "    #now dataSplit is containing the n_folds data\n",
    "\n",
    "\n",
    "    for k in k_values:\n",
    "        print(\"k is \"+str(k))\n",
    "        scores = list()\n",
    "        for i in range(len(dataSplit)):\n",
    "            \n",
    "            trainSet = list(dataSplit)\n",
    "            ySet = list(y_split)\n",
    "\n",
    "            f1 = trainSet.pop(i)\n",
    "            f2 =  ySet.pop(i) #keep the current fold for test, others for the training\n",
    "           \n",
    "\n",
    "            # trainSet = sum(trainSet, [])\n",
    "\n",
    "            # print(trainSet)\n",
    "\n",
    "            testSet = np.reshape(f1,(foldSize,X.shape[1]))\n",
    "            testLabel = np.reshape(f2,(foldSize,))\n",
    "\n",
    "            print(trainSet)\n",
    "            print(ySet)\n",
    "            print(f1.shape)\n",
    "\n",
    "        #     #now fit the train set \n",
    "        #     trainLabels = [row[-1] for row in trainSet]\n",
    "        #     trainSet = [train[:-1] for train in trainSet]\n",
    "            estimator.fit(trainSet,ySet)     \n",
    "\n",
    "        #     # and test on the current fold\n",
    "        #     actual = [row[-1] for row in testSet] #the actual label \n",
    "        #     testSet = [test[:-1] for test in testSet]\n",
    "\n",
    "            pred = estimator.predict(f1)\n",
    "            print(pred)\n",
    "                \n",
    "        #     accuracy = printMetrics(actual, pred)\n",
    "        #     scores.append(accuracy)\n",
    "\n",
    "        # print(\"for k = %d\" % k)\n",
    "        # print('Scores %s' % scores)\n",
    "        # print('-'*20)        \n",
    "        # print('\\nMaximum Accuracy: %3f%%' % max(scores))\n",
    "        # print('\\nMean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))\n",
    "\n",
    "def plot_learning_curve(k, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Plot the learning curve for a KNN classifier using different values of k.\n",
    "\n",
    "    NOTE: You can modify arguments as you like, except `k`.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    k : int or array like\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
